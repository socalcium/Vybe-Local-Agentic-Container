[
  {
    "name": "llama3.2",
    "tag": "3b",
    "description": "Latest Meta Llama 3.2 model optimized for efficiency and performance. Excellent for general chat, reasoning, and creative tasks with improved safety alignment.",
    "fidelity": "High",
    "pc_load": "Low",
    "categories": ["Chat", "Reasoning", "Creative", "Assistant"],
    "uncensored": false,
    "n_ctx": 128000,
    "size_gb": 2.0,
    "languages": ["English", "Spanish", "French", "German", "Italian", "Portuguese", "Hindi", "Thai"],
    "strengths": ["Conversational AI", "Creative writing", "Code assistance", "Analysis"],
    "use_cases": ["General chat", "Content creation", "Q&A", "Light coding help"]
  },
  {
    "name": "llama3.2",
    "tag": "1b",
    "description": "Ultra-lightweight Llama 3.2 for mobile and edge deployment. Maintains surprising capability despite compact size.",
    "fidelity": "Medium-High",
    "pc_load": "Very Low",
    "categories": ["Chat", "Mobile", "Edge", "Basic"],
    "uncensored": false,
    "n_ctx": 128000,
    "size_gb": 1.3,
    "languages": ["English", "Spanish", "French", "German"],
    "strengths": ["Speed", "Low memory usage", "Mobile deployment"],
    "use_cases": ["Quick responses", "Mobile apps", "Edge computing", "Basic assistance"]
  },
  {
    "name": "llama3.1",
    "tag": "8b",
    "description": "Enhanced version of Llama3 with massive 128K context window. Exceptional for long-form reasoning, document analysis, and complex conversations.",
    "fidelity": "Very High",
    "pc_load": "Medium",
    "categories": ["Chat", "Coding", "Reasoning", "Analysis"],
    "uncensored": false,
    "n_ctx": 131072,
    "size_gb": 4.7,
    "languages": ["English", "Spanish", "French", "German", "Italian", "Portuguese", "Hindi", "Thai"],
    "strengths": ["Long context", "Complex reasoning", "Code generation", "Document analysis"],
    "use_cases": ["Long documents", "Complex analysis", "Advanced coding", "Research assistance"]
  },
  {
    "name": "llama3.1",
    "tag": "70b",
    "description": "Flagship Llama 3.1 model with exceptional performance across all domains. Near GPT-4 level capabilities for users with powerful hardware.",
    "fidelity": "Exceptional",
    "pc_load": "Very High",
    "categories": ["Chat", "Coding", "Reasoning", "Creative", "Professional"],
    "uncensored": false,
    "n_ctx": 131072,
    "size_gb": 40.0,
    "languages": ["English", "Spanish", "French", "German", "Italian", "Portuguese", "Hindi", "Thai", "Chinese", "Japanese"],
    "strengths": ["Top-tier reasoning", "Advanced coding", "Creative writing", "Professional tasks"],
    "use_cases": ["Professional work", "Complex projects", "Advanced coding", "Research"]
  },
  {
    "name": "qwen2.5",
    "tag": "7b",
    "description": "Alibaba's latest Qwen model with exceptional multilingual capabilities and strong coding performance. Excellent balance of size and capability.",
    "fidelity": "High",
    "pc_load": "Medium",
    "categories": ["Chat", "Multilingual", "Coding", "Math"],
    "uncensored": false,
    "n_ctx": 32768,
    "size_gb": 4.3,
    "languages": ["English", "Chinese", "Japanese", "Korean", "Spanish", "French", "Russian", "Arabic"],
    "strengths": ["Multilingual", "Mathematics", "Code generation", "Reasoning"],
    "use_cases": ["International projects", "Math problems", "Code review", "Translation"]
  },
  {
    "name": "qwen2.5",
    "tag": "14b",
    "description": "Larger Qwen 2.5 with enhanced reasoning and coding capabilities. Exceptional for technical tasks and complex problem solving.",
    "fidelity": "Very High", 
    "pc_load": "High",
    "categories": ["Chat", "Multilingual", "Coding", "Math", "Reasoning"],
    "uncensored": false,
    "n_ctx": 32768,
    "size_gb": 8.7,
    "languages": ["English", "Chinese", "Japanese", "Korean", "Spanish", "French", "Russian", "Arabic"],
    "strengths": ["Advanced reasoning", "Complex coding", "Mathematical proofs", "Research"],
    "use_cases": ["Scientific computing", "Advanced programming", "Research", "Technical writing"]
  },
  {
    "name": "qwen2.5-coder",
    "tag": "7b",
    "description": "Specialized Qwen model trained specifically for code generation, debugging, and software development tasks.",
    "fidelity": "High",
    "pc_load": "Medium",
    "categories": ["Coding", "Programming", "Debug", "DevOps"],
    "uncensored": false,
    "n_ctx": 32768,
    "size_gb": 4.3,
    "languages": ["English", "Chinese"],
    "strengths": ["Code generation", "Bug fixing", "Code review", "Multiple languages"],
    "use_cases": ["Software development", "Code review", "Bug fixing", "API development"]
  },
  {
    "name": "mistral",
    "tag": "7b",
    "description": "Mistral AI's flagship 7B model with excellent instruction following and reasoning. High-quality European alternative to American models.",
    "fidelity": "High",
    "pc_load": "Medium",
    "categories": ["Chat", "Reasoning", "Creative", "Multilingual"],
    "uncensored": false,
    "n_ctx": 32768,
    "size_gb": 4.1,
    "languages": ["English", "French", "German", "Spanish", "Italian"],
    "strengths": ["Instruction following", "Creative writing", "Reasoning", "European languages"],
    "use_cases": ["General assistance", "Creative projects", "European language tasks", "Professional writing"]
  },
  {
    "name": "mixtral",
    "tag": "8x7b",
    "description": "Mistral's Mixture of Experts model delivering 70B-class performance at 7B inference cost. Revolutionary efficiency for high-end tasks.",
    "fidelity": "Exceptional",
    "pc_load": "Medium-High",
    "categories": ["Chat", "Reasoning", "Creative", "Professional"],
    "uncensored": false,
    "n_ctx": 32768,
    "size_gb": 26.9,
    "languages": ["English", "French", "German", "Spanish", "Italian"],
    "strengths": ["Mixture of Experts", "High performance", "Efficiency", "Complex reasoning"],
    "use_cases": ["Professional tasks", "Complex analysis", "High-quality content", "Research"]
  },
  {
    "name": "codellama",
    "tag": "7b",
    "description": "Meta's specialized code generation model based on Llama 2. Excellent for programming tasks across multiple languages.",
    "fidelity": "High",
    "pc_load": "Medium",
    "categories": ["Coding", "Programming"],
    "uncensored": false,
    "n_ctx": 16384,
    "size_gb": 3.8,
    "languages": ["English"],
    "strengths": ["Code generation", "Multiple programming languages", "Code completion", "Debugging"],
    "use_cases": ["Software development", "Code generation", "Programming help", "Technical documentation"]
  },
  {
    "name": "codellama",
    "tag": "13b",
    "description": "Larger CodeLlama with enhanced capabilities for complex programming tasks and architectural decisions.",
    "fidelity": "Very High",
    "pc_load": "High",
    "categories": ["Coding", "Programming", "Architecture"],
    "uncensored": false,
    "n_ctx": 16384,
    "size_gb": 7.3,
    "languages": ["English"],
    "strengths": ["Complex programming", "System design", "Code architecture", "Advanced debugging"],
    "use_cases": ["System architecture", "Complex algorithms", "Code review", "Technical leadership"]
  },
  {
    "name": "phi3",
    "tag": "mini",
    "description": "Microsoft's compact yet powerful model. Exceptional performance per parameter with enterprise-grade safety and efficiency.",
    "fidelity": "High",
    "pc_load": "Very Low",
    "categories": ["Chat", "Summarization", "Business", "Efficiency"],
    "uncensored": false,
    "n_ctx": 128000,
    "size_gb": 2.3,
    "languages": ["English"],
    "strengths": ["Efficiency", "Enterprise safety", "Summarization", "Business tasks"],
    "use_cases": ["Business automation", "Document processing", "Quick responses", "Enterprise apps"]
  },
  {
    "name": "phi3",
    "tag": "medium",
    "description": "Balanced Phi-3 model offering strong performance with reasonable resource requirements. Great for business applications.",
    "fidelity": "Very High",
    "pc_load": "Medium",
    "categories": ["Chat", "Business", "Analysis", "Reasoning"],
    "uncensored": false,
    "n_ctx": 128000,
    "size_gb": 7.9,
    "languages": ["English"],
    "strengths": ["Business intelligence", "Analysis", "Enterprise safety", "Reasoning"],
    "use_cases": ["Business analysis", "Professional writing", "Data interpretation", "Strategic planning"]
  },
  {
    "name": "gemma2",
    "tag": "9b",
    "description": "Google's latest Gemma model with improved reasoning, safety, and efficiency. Strong performance across diverse tasks.",
    "fidelity": "High",
    "pc_load": "Medium",
    "categories": ["Chat", "Reasoning", "Creative", "Safe"],
    "uncensored": false,
    "n_ctx": 8192,
    "size_gb": 5.4,
    "languages": ["English"],
    "strengths": ["Safety", "Reasoning", "Factual accuracy", "Versatility"],
    "use_cases": ["Educational content", "Safe applications", "General assistance", "Creative writing"]
  },
  {
    "name": "gemma2",
    "tag": "27b",
    "description": "Large Gemma 2 model with exceptional capabilities and Google's advanced safety features. Top-tier performance for demanding tasks.",
    "fidelity": "Exceptional",
    "pc_load": "Very High",
    "categories": ["Chat", "Reasoning", "Creative", "Professional", "Safe"],
    "uncensored": false,
    "n_ctx": 8192,
    "size_gb": 16.0,
    "languages": ["English"],
    "strengths": ["Advanced reasoning", "Safety", "Complex tasks", "High accuracy"],
    "use_cases": ["Professional applications", "Research", "Complex analysis", "High-stakes tasks"]
  },
  {
    "name": "deepseek-coder",
    "tag": "6.7b",
    "description": "Specialized coding model with exceptional programming capabilities across multiple languages and frameworks.",
    "fidelity": "Very High",
    "pc_load": "Medium",
    "categories": ["Coding", "Programming", "DevOps"],
    "uncensored": false,
    "n_ctx": 16384,
    "size_gb": 3.7,
    "languages": ["English"],
    "strengths": ["Code generation", "Multiple languages", "Best practices", "Code optimization"],
    "use_cases": ["Software development", "Code optimization", "Best practices", "Technical documentation"]
  },
  {
    "name": "nous-hermes2",
    "tag": "7b",
    "description": "Fine-tuned assistant model with excellent instruction following and helpful, harmless responses. Popular community choice.",
    "fidelity": "High",
    "pc_load": "Medium",
    "categories": ["Chat", "Assistant", "Reasoning", "Helpful"],
    "uncensored": false,
    "n_ctx": 8192,
    "size_gb": 4.1,
    "languages": ["English"],
    "strengths": ["Instruction following", "Helpfulness", "Reasoning", "Versatility"],
    "use_cases": ["General assistance", "Q&A", "Problem solving", "Educational support"]
  },
  {
    "name": "nous-hermes2",
    "tag": "mixtral",
    "description": "Community fine-tune of Mixtral with enhanced assistant capabilities and instruction following.",
    "fidelity": "Exceptional",
    "pc_load": "High",
    "categories": ["Chat", "Assistant", "Reasoning", "Professional"],
    "uncensored": false,
    "n_ctx": 32768,
    "size_gb": 26.9,
    "languages": ["English"],
    "strengths": ["Advanced reasoning", "Professional assistance", "Complex tasks", "High quality"],
    "use_cases": ["Professional work", "Complex problem solving", "Research assistance", "High-level consultation"]
  },
  {
    "name": "tinyllama",
    "tag": "1.1b",
    "description": "Ultra-lightweight model perfect for resource-constrained environments, IoT devices, and mobile applications.",
    "fidelity": "Medium",
    "pc_load": "Very Low",
    "categories": ["Chat", "Basic", "Mobile", "IoT"],
    "uncensored": false,
    "n_ctx": 2048,
    "size_gb": 0.6,
    "languages": ["English"],
    "strengths": ["Ultra-low resource usage", "Fast responses", "Mobile deployment", "IoT applications"],
    "use_cases": ["Embedded systems", "Mobile apps", "Quick responses", "Resource-limited environments"]
  },
  {
    "name": "dolphin-mixtral",
    "tag": "8x7b",
    "description": "Uncensored fine-tune of Mixtral with enhanced creative capabilities and reduced safety restrictions. For advanced users only.",
    "fidelity": "Exceptional",
    "pc_load": "High",
    "categories": ["Chat", "Creative", "Uncensored", "Advanced"],
    "uncensored": true,
    "n_ctx": 32768,
    "size_gb": 26.9,
    "languages": ["English"],
    "strengths": ["Creative freedom", "Uncensored responses", "High capability", "Advanced reasoning"],
    "use_cases": ["Creative writing", "Advanced research", "Unrestricted tasks", "Adult applications"]
  },
  {
    "name": "solar",
    "tag": "10.7b",
    "description": "Upstage's SOLAR model with unique depth upscaling architecture delivering exceptional performance for its size.",
    "fidelity": "Very High",
    "pc_load": "Medium-High",
    "categories": ["Chat", "Reasoning", "Korean", "Multilingual"],
    "uncensored": false,
    "n_ctx": 4096,
    "size_gb": 6.1,
    "languages": ["English", "Korean"],
    "strengths": ["Unique architecture", "High performance", "Korean language", "Reasoning"],
    "use_cases": ["Korean language tasks", "Research", "Bilingual applications", "High-quality chat"]
  },
  {
    "name": "starling",
    "tag": "7b",
    "description": "High-performance model trained with RLAIF (Reinforcement Learning from AI Feedback) for exceptional helpfulness and accuracy.",
    "fidelity": "Very High",
    "pc_load": "Medium",
    "categories": ["Chat", "Assistant", "Helpful", "Accurate"],
    "uncensored": false,
    "n_ctx": 8192,
    "size_gb": 4.1,
    "languages": ["English"],
    "strengths": ["RLAIF training", "High helpfulness", "Accuracy", "Reliability"],
    "use_cases": ["Professional assistance", "Accurate information", "Reliable responses", "Quality assurance"]
  },
  {
    "name": "dolphin-llama3",
    "tag": "8b",
    "description": "Uncensored fine-tune of Llama 3 with enhanced reasoning and reduced safety restrictions. Large 32K context window for complex tasks.",
    "fidelity": "Very High",
    "pc_load": "Medium",
    "categories": ["Chat", "Uncensored", "Reasoning", "Creative"],
    "uncensored": true,
    "n_ctx": 32768,
    "size_gb": 4.7,
    "languages": ["English"],
    "strengths": ["Uncensored responses", "Large context", "Creative freedom", "Advanced reasoning"],
    "use_cases": ["Creative writing", "Research", "Unrestricted conversations", "Adult content"]
  },
  {
    "name": "wizard-vicuna-uncensored",
    "tag": "13b",
    "description": "Uncensored merge of WizardLM and Vicuna with exceptional instruction following and creative capabilities. 16K context window.",
    "fidelity": "Very High",
    "pc_load": "High",
    "categories": ["Chat", "Uncensored", "Creative", "Instructions"],
    "uncensored": true,
    "n_ctx": 16384,
    "size_gb": 7.3,
    "languages": ["English"],
    "strengths": ["Uncensored", "Instruction following", "Creative tasks", "Problem solving"],
    "use_cases": ["Creative projects", "Research assistance", "Unrestricted tasks", "Advanced conversations"]
  },
  {
    "name": "openhermes",
    "tag": "2.5-mistral-7b",
    "description": "Uncensored Hermes fine-tune of Mistral with excellent instruction following and 32K context. Balanced performance and freedom.",
    "fidelity": "Very High",
    "pc_load": "Medium",
    "categories": ["Chat", "Uncensored", "Instructions", "Balanced"],
    "uncensored": true,
    "n_ctx": 32768,
    "size_gb": 4.1,
    "languages": ["English"],
    "strengths": ["Instruction following", "Uncensored", "Large context", "Balanced responses"],
    "use_cases": ["General assistance", "Creative writing", "Research", "Technical tasks"]
  },
  {
    "name": "llama3-uncensored",
    "tag": "8b-128k",
    "description": "Abliterated Llama 3 with massive 128K context window and removed safety restrictions. Ultimate freedom for advanced users.",
    "fidelity": "Exceptional",
    "pc_load": "Medium-High",
    "categories": ["Chat", "Uncensored", "Long-Context", "Advanced"],
    "uncensored": true,
    "n_ctx": 131072,
    "size_gb": 4.7,
    "languages": ["English"],
    "strengths": ["Massive context", "Uncensored", "Advanced reasoning", "Document analysis"],
    "use_cases": ["Long document analysis", "Unrestricted research", "Complex projects", "Advanced conversations"]
  },
  {
    "name": "mixtral-uncensored",
    "tag": "8x7b-32k",
    "description": "Uncensored Mixtral with 32K context and mixture of experts architecture. Top-tier performance with complete freedom.",
    "fidelity": "Exceptional",
    "pc_load": "Very High",
    "categories": ["Chat", "Uncensored", "Expert", "Professional"],
    "uncensored": true,
    "n_ctx": 32768,
    "size_gb": 26.9,
    "languages": ["English", "French", "Spanish", "German"],
    "strengths": ["Expert-level reasoning", "Uncensored", "Multilingual", "Professional capabilities"],
    "use_cases": ["Professional research", "Complex analysis", "Multilingual tasks", "High-level conversations"]
  }
]
